# 오전

- ## 자연어 처리 딥러닝 모델 변천사

  - LSTM : 복잡도가 낮다보니 한계가 발생
  - CNN + LSTM
  - Seq2seq : 알고리즘 체계가 달라짐. 모델을 이용한 특수한 작업가능(빈칸추론가능), 옛날 챗봇 모델.
  - Transformer : LLM을 구현하는데 사용됨. 문맥을 파악하기 시작함.
    - BERT
    - GPT

- ## Embedding Vector

  - 원핫벡터
  - 임베딩 벡터 : 원하는 차원으로 제한할 수 있음.
    - LLM 모델은 아직 데이터를 때려 넣는 게 좋다.
  - 단어 벡터 간 유의미한 유사도 : 코사인 유사도만 사용

- ## Word2Vec :

  - CBOW
    - 주변 단어를 통해 중심 단어를 예측하는 방법으로 학습
    - 중심단어에 따라 정확도가 달라질 수 있음. 사용하는 주변 단어의 수에 따라 정확도가 달라질 수 있음.
  - Skip-gram
    - 중심 단어를 통해 주변단어를 예측하는 방법으로 학습
    - 앞뒤에 뭐가 들어올지 생성하는 쪽에 가까움

- ## transformer

  - Residual connection : 다음 단계로 뛰어넘는 것
  - layer normalization

- ## 코사인유사도
  - 0, 360도 : y는 1/ 180도 : y는 -1

# 오후

- ## 전일치 검색
