# 오전

- ## 회귀

  - 회귀의 유형

    - 경사하강법
    - 확률적 경사하강법 : 한번의 스텝에 하나의 데이터에 대한 예측값을 실행한 후에 그 결과를 이용하여 그래디언트를 계한하고 파라미터를 조정한다
    - 미니배치경사하강법 (성능개선을 위함) / 로컬미니마에 수렴할 확률이 커짐(스텝이 많아서)

  - learning schedule

    - 학습스케쥴 : 큰 학습률에서 시작하고 학습속도가 느려질때 학습률을 낮추면 최적의 고정학습률보다 더 좋은 솔루션을 빨리 발견할 수 있다

  - 다항 회귀

    - 비선형데이터를 선형회귀를 이용하여 학습하는 기법

  - learning curve

    - RMSE : 평균 제곱근 오차/ mse에 루트를 씌운 값
    - 과대적합:트레인셋 성능지표가 valid보다 높은 경우

  - 회귀모델

    - 로지스틱 회귀

    - 회귀모델을 분류모델로 활용?
    - 이진분류 : 로지스틱 회귀
    - 다중 클래스 분류 : 소프트맥스 회귀
    - 선형회귀 모델이 예측한 값에 시그모이드 함수를 적용하여 0과 1 사이의 값, 즉 양성일 확률로 지정한다
    - 학습과 비용함수

    - 소프트맥스 회귀
    - 로지스틱 회귀모델을 일반화하여 다중 클래스 분류를 지원하도록 만든 모델

- ## 차원 축소

  - 차원이 증가할수록 데이터 간 빈 공간이 생기게 됨으로써 생기는 문제들을 차원의 저주라 칭함.
    - 차원이 커질수록 임의의 두 지점 사이의 평균 거리가 매우 멀어진다
    - 과대적합의 위험도가 커진다
  - 내가 필요한 데이터보다 복잡해지면 안좋음
  - 차원이 늘어날때마다 계산해야할 것은 거듭제곱만큼 늘어남
  - 차원의 저주 문제를 해결하기 위해 특성 수를 줄여서 학습 불가능한 문제를 학습 가능한 문제로 만드는 기법
    => 즉 정보의 손실이 크지 않은 방향으로 고차원의 데이터를 저차원의 데이터셋으로 변환시키는 것
  - PCA(주성분분석)

    - 학습 데이터셋을 특정 초평면(3차원 이상에 존재하는 저차원의 공간)에 사영하는 기법
    - 학습 데이터 셋을 특정 초평면에 사영하는 기법
    - 주성분끼리는 서로 수직을 이루도록 한다.
    - 차원이 늘어남에 따라 설명된 분산은 1에 수렴해짐
    - 활용
      - 글자 데이터 차원축소 -> 해상도가 낮아짐
      - 패키지를 돌릴때 그룹이 나눠지면 이는 전체를 대상으로 나눈 것이기에 구체적으로 어떤게 들어갔는지는 알 수 없음.

  - LDA(산형 판별 분석)

    - PCA는 입력 데이터의 변동성이 가장 큰 축을 찾음(분산분포)
    - LDA는 입력 데이터의 결정값 클래스를 최대한 분리할 수 있는 축을 찾음

  - 차원 축소 모델의 차이
    - MDS
    - Linear Kernel
    - RBF kernel
    - Isomap (차원을 최대한 많이 줄이려면 가장 적합)
    - t-SNE
    - sigmoid kernel

# 오후

- ## 비지도 학습

  - 비지도학습 : 레이블이 없는 데이터를 학습하는 기법

  - 군집화 : 비슷한 샘플끼리의 군집을 형성하는 것이며 아래의 용도로 사용된다

    - 데이터분석
    - 고객분류
    - 추천 시스템
    - 검색 엔진
    - 이미지 분할
    - 준지도 학습

  - 이상치 탐지 : 정상 데이터와 이상치를 구분하는데 활용된다.

    - 생산라인

  - 종류

    - 군집화(clustering)

      - 군집: 유사한 데이터들의 모음

      - 군집화 : 데이터 포인트들의 별개의 군집으로 그룹화하는 것

      - 유사성이 높은 데이터들을 동일한 그룹으로 분류하고 서로 다른 군집들이 상이성을 가지도록 그룹화한다.

      - 필요한 파라미터 : 시작점, 얼마나 근처점을 찾을 것인가, 어느정도 떨어져있으면 다른 군집으로 생각할 것인가

      - 활용 분야

        - 고객 분류

      - 알고리즘

        - **K-means** : 근처점을 찾는 알고리즘(k개만큼)

          - 내가 몇개의 클러스터로 나누겠다 => 적당한 중심점을 찍어줌
          - 장점 : 일반적으로 가장 많이 활용/ 쉽고간결/ 대용량 데이터에도 활용 가능
          - 단점 : 초기 센트로이드 위치에 따라 성능이 달리짐/ 군집 개수를 직접 정해야 함/ 반복 횟수가 많을 경우 수행시간이 느려진다/ 이상치에 취약하다
          - 최적의 군집수 : 군집수가 적절하지 않으면 좋지 않은 모델로 수렴할 수 있다 => 실루엣 계수
          - 실루엣 계수 : 같은 그룹 내에서 각 점과 다른 점 사이의 평균을 a로 둠. a그룹에서 다른 그룹으로 선을 그었을 때 평균을 b라고 둠. (b-a)/max(a,b)
            - 같은 그룹에선 가깝게 다른 그룹에서 멀리나오는게 가장 베스트(점수가 1이면 가장 좋음/0이면 안 좋음)
            - 유사성이 높은 데이터들을 동일한 그룹으로 분류하고 서로 다른 군집들이 상이성을 가지도록 그룹화한다.
            - 각 데이터의 실루엣 계수들을 모아놓은 그래프, 실루엣 다이어그램
              - 군집 개수별로 데이터들의 실루엣 계수들의 시각화
              - 평균만 높다고 좋은 군집이 아니라 군집별 크기가 비슷해야 좋은 모델

        - **Hierarchical Clustering** : 계층적 클러스터링

          - 데이터를 하나하나 계층에 따라 순차적으로 클러스터링 하는 기법
            - 클러스터의 개수를 미리 지정할 필요가 없다.

        - **DBSCAN** : 특정점에서 가장 가까운 특성을 가지는 점을 찾고 또 가까운 특성을 가진 점을 찾아나가는 것

          - 클러스터의 개수를 미리 지정할 필요가 없다
          - 이상치를 효과적으로 제외할 수 있다
          - 특정 좌표의 샘플이 있을때, 가장 가까운 군집을 찾아냄
          - 장점 : 간단하면서 강력/ 군집의 모양과 개수에 상관없다/ 이상치에 안정적이다
          - 단점 : 군집간 밀집도가 크게 다르면 모든 군집 파악이 불가능

        - **Gaussian Mixture Model** : 모든 집단은 정규분포를 가진 집단의 혼합이다.
          - 데이터셋이 여러 개의 혼합된 가우시안 분포를 따르는 샘플들로 구성되었다고 가정
          - 실제 많이 떨어졌는가는 해당 모델에서는 그렇게 중요한 요소가 아님
