{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DBconnector.py & setting.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'psycopg2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnector\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DBconnector\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msettings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DB_SETTINGS\n",
      "File \u001b[1;32mc:\\Users\\dongi\\OneDrive\\바탕 화면\\chunjae\\11_Engineering\\DAY2\\db\\connector.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpsycopg2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpsycopg2\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpgsql_query\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpostgresql_qurey\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'psycopg2'"
     ]
    }
   ],
   "source": [
    "from db.connector import DBconnector\n",
    "from settings import DB_SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'host': '127.0.0.1',\n",
       " 'database': 'postgres',\n",
       " 'user': 'postgres',\n",
       " 'password': '1234',\n",
       " 'port': '5432'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DB_SETTINGS[\"POSTGRES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter\n",
      "[(6, 'Margaret', 1880, 'F', 1578), (7, 'Ida', 1880, 'F', 1472), (8, 'Alice', 1880, 'F', 1414), (9, 'Bertha', 1880, 'F', 1320), (10, 'Sarah', 1880, 'F', 1288)]\n",
      "Exit\n"
     ]
    }
   ],
   "source": [
    "from db.connector import DBconnector\n",
    "from settings import DB_SETTINGS\n",
    "\n",
    "db_connector = DBconnector(**DB_SETTINGS[\"POSTGRES\"])\n",
    "\n",
    "with db_connector as connected:\n",
    "    conn = connected.conn\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\"SELECT * FROM lecture LiMIT 5\")\n",
    "    print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\user\\\\Desktop\\\\Memo\\\\1028\\\\천재교육\\\\DAY2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(r\"c:\\\\Users\\\\user\\\\Desktop\\\\Memo\\\\1028\\\\천재교육\\\\DAY2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. query.py\n",
    "\n",
    "    - 쿼리들은 파일로 관리하여 쉽게 호출할 수 있도록 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 쿼리 내용 조회하는 부분을 class 내에 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import db.pgsql_query as postgresql_qurey\n",
    "from settings import DB_SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBconnector:\n",
    "    def __init__(self, host, database, user, password, port):\n",
    "        self.conn_params = dict(\n",
    "            host=host, dbname=database, user=user, password=password, port=port\n",
    "        )\n",
    "\n",
    "        self.connect = self.postgres_connect()\n",
    "        self.queries = postgresql_qurey.queries\n",
    "\n",
    "    def __enter__(self):\n",
    "        print(\"Enter\")\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exe_type, exe_value, traceback):\n",
    "        self.conn.close()\n",
    "        print(\"Exit\")\n",
    "\n",
    "    def postgres_connect(self):\n",
    "        self.conn = psycopg2.connect(**self.conn_params)\n",
    "        return self\n",
    "\n",
    "    def get_query(self, table_name):\n",
    "        try:\n",
    "            _query = self.queries[table_name]\n",
    "            return _query\n",
    "        except KeyError:\n",
    "            raise KeyError(\n",
    "                f\"'{table_name}' 키가 queries 에 존재하지 않습니다. 현재 있는 키 리스트 : {list(self.queries.keys)}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT * FROM lecture'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_connector = DBconnector(**DB_SETTINGS[\"POSTGRES\"])\n",
    "\n",
    "db_connector.get_query(\"lecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM lecture\n",
      "SELECT * FROM tbl LIMIT 5\n"
     ]
    }
   ],
   "source": [
    "from db.pgsql_query import queries\n",
    "\n",
    "for tbl in queries.keys():\n",
    "    db_connector = DBconnector(**DB_SETTINGS[\"POSTGRES\"])\n",
    "    _query = db_connector.get_query(tbl)\n",
    "    print(_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract.py\n",
    "- 쿼리를 받아 DB에 조회하여 결과를 pandas dataframe으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db.connector import DBconnector\n",
    "from settings import DB_SETTINGS\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter\n",
      "Exit\n",
      "    id       name  year gender  count\n",
      "0    6   Margaret  1880      F   1578\n",
      "1    7        Ida  1880      F   1472\n",
      "2    8      Alice  1880      F   1414\n",
      "3    9     Bertha  1880      F   1320\n",
      "4   10      Sarah  1880      F   1288\n",
      "5   11      Annie  1880      F   1258\n",
      "6   12      Clara  1880      F   1226\n",
      "7   13       Ella  1880      F   1156\n",
      "8   14   Florence  1880      F   1063\n",
      "9   15       Cora  1880      F   1045\n",
      "10  16     Martha  1880      F   1040\n",
      "11  17      Laura  1880      F   1012\n",
      "12  18     Nellie  1880      F    995\n",
      "13  19      Grace  1880      F    982\n",
      "14  20     Carrie  1880      F    949\n",
      "15  21      Maude  1880      F    858\n",
      "16  22      Mabel  1880      F    808\n",
      "17  23     Bessie  1880      F    796\n",
      "18  24     Jennie  1880      F    793\n",
      "19  25   Gertrude  1880      F    787\n",
      "20  26      Julia  1880      F    783\n",
      "21  27     Hattie  1880      F    769\n",
      "22  28      Edith  1880      F    768\n",
      "23  29     Mattie  1880      F    704\n",
      "24  30       Rose  1880      F    700\n",
      "25  31  Catherine  1880      F    688\n",
      "26  32    Lillian  1880      F    672\n",
      "27  33        Ada  1880      F    652\n",
      "28  34     Lillie  1880      F    647\n",
      "29  35      Helen  1880      F    636\n",
      "30  36     Jessie  1880      F    635\n",
      "31  37     Louise  1880      F    635\n",
      "32  38      Ethel  1880      F    633\n",
      "33  39       Lula  1880      F    621\n",
      "34  40     Myrtle  1880      F    615\n",
      "35  41        Eva  1880      F    614\n",
      "36  42    Frances  1880      F    605\n",
      "37  43       Lena  1880      F    603\n",
      "38  44       Lucy  1880      F    590\n",
      "39  45       Edna  1880      F    588\n",
      "40  46     Maggie  1880      F    582\n",
      "41  47      Pearl  1880      F    569\n",
      "42  48      Daisy  1880      F    564\n",
      "43  49     Fannie  1880      F    560\n",
      "44  50  Josephine  1880      F    544\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_22868\\602862886.py:6: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(_query, con)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_connector = DBconnector(**DB_SETTINGS[\"POSTGRES\"])\n",
    "\n",
    "with db_connector as connected:\n",
    "    _query = db_connector.get_query(\"lecture\")\n",
    "    con = connected.conn\n",
    "    df = pd.read_sql(_query, con)\n",
    "\n",
    "print(df), print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractor(db_connector, table_name):\n",
    "    with db_connector as connected:\n",
    "        try:\n",
    "            _query = connected.get_query(table_name)\n",
    "            con = connected.conn\n",
    "            df = pd.read_sql(_query, con)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Extract MSG: {e}\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter\n",
      "Exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_18896\\3118118505.py:6: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(_query, con)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>year</th>\n",
       "      <th>gender</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>Margaret</td>\n",
       "      <td>1880</td>\n",
       "      <td>F</td>\n",
       "      <td>1578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Ida</td>\n",
       "      <td>1880</td>\n",
       "      <td>F</td>\n",
       "      <td>1472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>Alice</td>\n",
       "      <td>1880</td>\n",
       "      <td>F</td>\n",
       "      <td>1414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>Bertha</td>\n",
       "      <td>1880</td>\n",
       "      <td>F</td>\n",
       "      <td>1320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>Sarah</td>\n",
       "      <td>1880</td>\n",
       "      <td>F</td>\n",
       "      <td>1288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id      name  year gender  count\n",
       "0   6  Margaret  1880      F   1578\n",
       "1   7       Ida  1880      F   1472\n",
       "2   8     Alice  1880      F   1414\n",
       "3   9    Bertha  1880      F   1320\n",
       "4  10     Sarah  1880      F   1288"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_connector = DBconnector(**DB_SETTINGS[\"POSTGRES\"])\n",
    "\n",
    "return_extractor = extractor(db_connector, \"lecture\")\n",
    "return_extractor.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. transform.py\n",
    "\n",
    "- Batch 날짜별 저장 경로 생성 및 해당 경로 이하에 df 저장\n",
    "- 이행 환경에 따라 다르게 구성될 수 있음\n",
    "    - Database -> Stragins Server -> Cloud/Database\n",
    "    - Database -- Directory Connection -> Cloud/Database\n",
    "\n",
    "- 목적지 database의 성격에 따라 추가적인 처리 함수가 포함될 수 있음.\n",
    "    - Data Lake -> 거의 가공 없이 이행\n",
    "    - Data Warehouse -> 결측치/ 공백 등 간단한 전처리를 거쳐 이행\n",
    "    - Data Mart -> Group by/filter 등 성격에 맞는 데이터 처리를 거쳐 이행\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 저장 경로 생성\n",
    "- Database 이름/ table 이름 / yyyy={}/ mm = {}/ dd = {}/ {table_name}.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter\n",
      "Exit\n",
      "   id      name  year gender  count\n",
      "0   6  Margaret  1880      F   1578\n",
      "1   7       Ida  1880      F   1472\n",
      "2   8     Alice  1880      F   1414\n",
      "3   9    Bertha  1880      F   1320\n",
      "4  10     Sarah  1880      F   1288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Desktop\\Memo\\1028\\천재교육\\DAY2\\pipeline\\extract.py:8: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(_query, con)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from db.connector import DBconnector\n",
    "from settings import DB_SETTINGS\n",
    "from pipeline.extract import extractor\n",
    "\n",
    "\n",
    "db_connector = DBconnector(**DB_SETTINGS[\"POSTGRES\"])\n",
    "table_name = \"lecture\"\n",
    "\n",
    "return_extractor = extractor(db_connector, table_name)\n",
    "# return_extractor.head()\n",
    "if isinstance(return_extractor, pd.DataFrame):\n",
    "    print(return_extractor.head())\n",
    "\n",
    "else:\n",
    "    print(\"데이터를 가져오지 못했습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch 날짜 설정\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "batch_date = datetime.now()\n",
    "format_date = batch_date.strftime(\"%Y%m%d\")\n",
    "\n",
    "_y = format_date[:4]\n",
    "_m = format_date[4:6]\n",
    "_d = format_date[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2024', '10', '28')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{batch_date:%Y}\", f\"{batch_date:%m}\", f\"{batch_date:%d}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\user\\\\Desktop\\\\Memo\\\\1028\\\\천재교육\\\\DAY2\\\\temp_storage\\\\postgres\\\\lecture'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "temp_path = \"c:\\\\Users\\\\user\\\\Desktop\\\\Memo\\\\1028\\\\천재교육\\\\DAY2\\\\temp_storage\"\n",
    "\n",
    "_path = os.path.join(temp_path, \"postgres\", \"lecture\")\n",
    "_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\user\\\\Desktop\\\\Memo\\\\1028\\\\천재교육\\\\DAY2'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) pandas dataframe을 csv/parquet 형태로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "batch_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "temp_path = \"c:\\\\Users\\\\user\\\\Desktop\\\\Memo\\\\1028\\\\천재교육\\\\DAY2\\\\temp_storage\"\n",
    "\n",
    "\n",
    "def create_path(temp_path, batch_date):\n",
    "    _y = format_date[:4]\n",
    "    _m = format_date[4:6]\n",
    "    _d = format_date[6:]\n",
    "\n",
    "    return _path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장 폴더 생성\n",
    "\n",
    "path = create_path(temp_path, batch_date)\n",
    "os.makedirs(path, mode=777, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV format\n",
    "save_path = os.path.join(path, \"lecture.csv\")\n",
    "save_path\n",
    "\n",
    "df.to_csv(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON format\n",
    "save_path = os.path.join(path, \"lecture.json\")\n",
    "save_path\n",
    "\n",
    "df.to_json(save_path, orient=\"records\", indent=4, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in c:\\users\\user\\anaconda3\\lib\\site-packages (14.0.2)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pyarrow) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\compat\\_optional.py:135\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 135\u001b[0m     module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(name)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\pyarrow\\__init__.py:65\u001b[0m\n\u001b[0;32m     64\u001b[0m _gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_lib\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _gc_enabled:\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing lib: 지정된 프로시저를 찾을 수 없습니다.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m save_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlecture.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m save_path\n\u001b[1;32m----> 6\u001b[0m df\u001b[38;5;241m.\u001b[39mto_parquet(save_path, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m'\u001b[39m, compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3113\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[1;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m   3032\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3033\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[0;32m   3034\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3109\u001b[0m \u001b[38;5;124;03m>>> content = f.read()\u001b[39;00m\n\u001b[0;32m   3110\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3111\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[1;32m-> 3113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m to_parquet(\n\u001b[0;32m   3114\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3115\u001b[0m     path,\n\u001b[0;32m   3116\u001b[0m     engine,\n\u001b[0;32m   3117\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   3118\u001b[0m     index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   3119\u001b[0m     partition_cols\u001b[38;5;241m=\u001b[39mpartition_cols,\n\u001b[0;32m   3120\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   3121\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3122\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\io\\parquet.py:476\u001b[0m, in \u001b[0;36mto_parquet\u001b[1;34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(partition_cols, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    475\u001b[0m     partition_cols \u001b[38;5;241m=\u001b[39m [partition_cols]\n\u001b[1;32m--> 476\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[0;32m    478\u001b[0m path_or_buf: FilePath \u001b[38;5;241m|\u001b[39m WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[0;32m    480\u001b[0m impl\u001b[38;5;241m.\u001b[39mwrite(\n\u001b[0;32m    481\u001b[0m     df,\n\u001b[0;32m    482\u001b[0m     path_or_buf,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\io\\parquet.py:78\u001b[0m, in \u001b[0;36mget_engine\u001b[1;34m(engine)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find a usable engine; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtried using: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfastparquet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     75\u001b[0m     )\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PyArrowImpl()\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfastparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m FastParquetImpl()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\io\\parquet.py:163\u001b[0m, in \u001b[0;36mPyArrowImpl.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     import_optional_dependency(\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m, extra\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow is required for parquet support.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    165\u001b[0m     )\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# import utils to register the pyarrow extension types\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\compat\\_optional.py:138\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 138\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# Handle submodules: if we have submodule, grab parent module from sys.modules\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow."
     ]
    }
   ],
   "source": [
    "# parguet format\n",
    "\n",
    "save_path = os.path.join(path, \"lecture.parquet\")\n",
    "save_path\n",
    "\n",
    "df.to_parquet(save_path, engine=\"pyarrow\", compression=\"gzip\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_file(df, path, table_name):\n",
    "    if len(df) > 0:\n",
    "        os.makedirs(path, mode=777)\n",
    "        save_path = os.path.join(path, f\"{table_name}.csv\")\n",
    "\n",
    "        df.to_csv(save_path)\n",
    "        return True\n",
    "\n",
    "    else:\n",
    "        print(\"EMPTY FILE\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_to_file(df, path, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 저장 경로 생성 + DataFrame 저장 함수 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer(create_path + save_to_file) 함수\n",
    "\n",
    "\n",
    "def transformer(temp_path, batch_date, df, table_name):\n",
    "    path = create_path(temp_path, batch_date)\n",
    "    res = save_to_file(df, path, table_name)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer(temp_path, batch_date, df, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db.connector import DBconnector\n",
    "from settings import DB_SETTINGS, TEMP_PATH\n",
    "from pipeline.extract import extractor\n",
    "from pipeline.transform import transformer\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter\n",
      "Exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Desktop\\Memo\\1028\\천재교육\\DAY2\\pipeline\\extract.py:8: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(_query, con)\n"
     ]
    }
   ],
   "source": [
    "db_connector = DBconnector(**DB_SETTINGS[\"POSTGRES\"])\n",
    "table_name = \"lecture\"\n",
    "batch_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "# print(batch_date)\n",
    "\n",
    "return_extractor = extractor(db_connector, table_name)\n",
    "return_extractor\n",
    "\n",
    "if return_extractor is not None and not return_extractor.empty:\n",
    "    retrun_transformer = transformer(\n",
    "        TEMP_PATH, batch_date, return_extractor, table_name\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\"DataFrame이 비었거나 데이터추출에 실패했습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. load.py\n",
    "- 저장된 파일을 특정한 저장소에 적재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) pandas to_sql() 메소드를 활용한 테이블 적재(Local File -> Database)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Engine(postgresql://postgres:***@127.0.0.1:5432/postgres)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = \"postgresql\"\n",
    "user = \"postgres\"\n",
    "password = \"1234\"\n",
    "host = \"127.0.0.1\"\n",
    "port = \"5432\"\n",
    "database = \"postgres\"\n",
    "\n",
    "db = create_engine(f\"{engine}://{user}:{password}@{host}:{port}/{database}\")\n",
    "\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>year</th>\n",
       "      <th>gender</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Mary</td>\n",
       "      <td>1880</td>\n",
       "      <td>F</td>\n",
       "      <td>7065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Anna</td>\n",
       "      <td>1880</td>\n",
       "      <td>F</td>\n",
       "      <td>2604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Emma</td>\n",
       "      <td>1880</td>\n",
       "      <td>F</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Elizabeth</td>\n",
       "      <td>1880</td>\n",
       "      <td>F</td>\n",
       "      <td>1939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Minnie</td>\n",
       "      <td>1880</td>\n",
       "      <td>F</td>\n",
       "      <td>1746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1996</td>\n",
       "      <td>Woodie</td>\n",
       "      <td>1880</td>\n",
       "      <td>M</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1997</td>\n",
       "      <td>Worthy</td>\n",
       "      <td>1880</td>\n",
       "      <td>M</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1998</td>\n",
       "      <td>Wright</td>\n",
       "      <td>1880</td>\n",
       "      <td>M</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1999</td>\n",
       "      <td>York</td>\n",
       "      <td>1880</td>\n",
       "      <td>M</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>2000</td>\n",
       "      <td>Zachariah</td>\n",
       "      <td>1880</td>\n",
       "      <td>M</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       name  year gender  count\n",
       "0        1       Mary  1880      F   7065\n",
       "1        2       Anna  1880      F   2604\n",
       "2        3       Emma  1880      F   2003\n",
       "3        4  Elizabeth  1880      F   1939\n",
       "4        5     Minnie  1880      F   1746\n",
       "...    ...        ...   ...    ...    ...\n",
       "1995  1996     Woodie  1880      M      5\n",
       "1996  1997     Worthy  1880      M      5\n",
       "1997  1998     Wright  1880      M      5\n",
       "1998  1999       York  1880      M      5\n",
       "1999  2000  Zachariah  1880      M      5\n",
       "\n",
       "[2000 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./dataset/data-01/names.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id         int64\n",
       "name      object\n",
       "year       int64\n",
       "gender    object\n",
       "count      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_sql(name=\"point\", con=db, if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(db_connector, db, table_name):\n",
    "    with db_connector as connected:\n",
    "        try:\n",
    "            orm_conn = connected.orm_conn\n",
    "            df.to_sql(name=table_name, con=orm_conn, if_exists=\"replace\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"loader Error MSG: {e}\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: sqlalchemy in c:\\users\\user\\anaconda3\\lib\\site-packages (1.4.54)\n",
      "Collecting sqlalchemy\n",
      "  Downloading SQLAlchemy-2.0.36-cp312-cp312-win_amd64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sqlalchemy) (4.11.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sqlalchemy) (3.0.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/11.5 MB 1.6 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 1.9/11.5 MB 24.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.5/11.5 MB 52.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.1/11.5 MB 108.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 81.8 MB/s eta 0:00:00\n",
      "Downloading SQLAlchemy-2.0.36-cp312-cp312-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.1/2.1 MB 64.9 MB/s eta 0:00:00\n",
      "Installing collected packages: sqlalchemy, pandas\n",
      "  Attempting uninstall: sqlalchemy\n",
      "    Found existing installation: SQLAlchemy 1.4.54\n",
      "    Uninstalling SQLAlchemy-1.4.54:\n",
      "      Successfully uninstalled SQLAlchemy-1.4.54\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.2\n",
      "    Uninstalling pandas-2.2.2:\n",
      "      Successfully uninstalled pandas-2.2.2\n",
      "Successfully installed pandas-2.2.3 sqlalchemy-2.0.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\user\\anaconda3\\Lib\\site-packages\\~qlalchemy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dataset 1.6.2 requires sqlalchemy<2.0.0,>=1.3.2, but you have sqlalchemy 2.0.36 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pandas sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from db.connector import DBconnector\n",
    "from settings import DB_SETTINGS, TEMP_PATH\n",
    "from pipeline.extract import extractor\n",
    "from pipeline.transform import transformer\n",
    "from pipeline.load import loader\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter\n",
      "Exit\n",
      "Enter\n",
      "loader Error MSG: 'bool' object has no attribute 'to_sql'\n",
      "Exit\n"
     ]
    }
   ],
   "source": [
    "db_connector = DBconnector(**DB_SETTINGS[\"POSTGRES\"])\n",
    "table_name = \"lecture\"\n",
    "batch_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "return_extractor = extractor(db_connector, table_name)\n",
    "# return_extractor\n",
    "\n",
    "if return_extractor is not None and not return_extractor.empty:\n",
    "    return_transformer = transformer(\n",
    "        TEMP_PATH, batch_date, return_extractor, table_name\n",
    "    )\n",
    "# return_transformer\n",
    "\n",
    "if return_extractor is not None and not return_extractor.empty:\n",
    "    return_loader = loader(db_connector, return_transformer, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "from settings import TEMP_PATH\n",
    "\n",
    "shutil.rmtree(TEMP_PATH)\n",
    "\n",
    "os.makedirs(TEMP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remover\n",
    "\n",
    "\n",
    "def remover(path):\n",
    "    try:\n",
    "        shutil.rmtree(path)\n",
    "        os.makedirs(path)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Remover Error MSG: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from db.connector import DBconnector\n",
    "from settings import DB_SETTINGS, TEMP_PATH\n",
    "from pipeline.extract import extractor\n",
    "from pipeline.transform import transformer\n",
    "from pipeline.load import loader\n",
    "from pipeline.remove import remover\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter\n",
      "Exit\n",
      "Enter\n",
      "Exit\n"
     ]
    }
   ],
   "source": [
    "db_connector = DBconnector(**DB_SETTINGS[\"POSTGRES\"])\n",
    "table_name = \"lecture\"\n",
    "batch_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "return_extractor = extractor(db_connector, table_name)\n",
    "\n",
    "if return_extractor is not None and not return_extractor.empty:\n",
    "    return_transformer = transformer(\n",
    "        TEMP_PATH, batch_date, return_extractor, table_name\n",
    "    )\n",
    "\n",
    "if return_transformer is not None and not return_transformer.empty:\n",
    "    return_loader = loader(db_connector, return_transformer, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remover(TEMP_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Controller\n",
    "- extractor, transformer 등 개별 모듈들에 대하여 순서대로 명령을 내려주는 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def controller():\n",
    "    \"\"\"\n",
    "    1. DBconnector >> DB Connector 생성\n",
    "    2. postgresql_query >> queries 에서 테이블 이름 목록(table_list) 받아오기\n",
    "        ex)\n",
    "            for tbl in table_list:\n",
    "\n",
    "    3. extract >> DB 조회 후 DataFrame 형태로 변환\n",
    "    4. transform >> 저장 경로 생성 후 임시 저장 디렉토리 아래에 dataframe 저장\n",
    "    5. load >> 저장소에 dataframe 파일 저장\n",
    "    6. remove >> 저장이 끝난 후 임시 저장 디렉토리 삭제\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db.connector import DBconnector\n",
    "from db.pgsql_query import queries\n",
    "from settings import DB_SETTINGS, TEMP_PATH\n",
    "from pipeline.extract import extractor\n",
    "from pipeline.transform import transformer\n",
    "from pipeline.load import loader\n",
    "from pipeline.remove import remover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파이썬 팁"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 클래스 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa', 'bb']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = {\"aa\": 1, \"bb\": 2}\n",
    "list(aa.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lecture': 'SELECT * FROM lecture', 'tbl': 'SELECT * FROM tbl LIMIT 5'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = __import__(\"db.pgsql_query\", fromlist=[\"\"])\n",
    "queries.queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NewYork', 1)]\n",
      "[('NewYork', 1), ('London', 20)]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "class cargo:\n",
    "    def __init__(self, capacity):\n",
    "        self.cargo = []\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def unload(self, port):\n",
    "        port_list = [p[0] for p in self.cargo]\n",
    "        if port in port_list:\n",
    "            unloaded = [i for i in self.cargo if i[0] == port]\n",
    "            return unloaded\n",
    "        else:\n",
    "            return\n",
    "\n",
    "    def can_depart(self):\n",
    "        _depart = True if sum([i[1] for i in self.cargo]) < -self.capacity else False\n",
    "        return _depart\n",
    "\n",
    "    def load(self, new_cargo):\n",
    "        self.cargo: list = new_cargo\n",
    "        pass\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ship = cargo(20)\n",
    "    ship.load([(\"NewYork\", 1), (\"London\", 20)])\n",
    "    print(ship.unload(\"NewYork\"))\n",
    "    print(ship.cargo)\n",
    "    print(ship.can_depart())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### json 가공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"name\": \"eggs\", \"price\": 1},{\"name\": \"rice\", \"price\": 4.04},{\"name\": \"coffee\", \"price\": 9.99}'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "aa = [\n",
    "    {\"name\": \"eggs\", \"price\": 1},\n",
    "    {\"name\": \"coffee\", \"price\": 9.99},\n",
    "    {\"name\": \"rice\", \"price\": 4.04},\n",
    "]\n",
    "aa\n",
    "\n",
    "sorted_items = sorted(aa, key=lambda x: (x[\"price\"], x[\"name\"]))\n",
    "joined_json = \",\".join([json.dumps(i) for i in sorted_items])\n",
    "# joined_json\n",
    "\n",
    "ret = \",\".join([json.dumps(i) for i in sorted_items])\n",
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'eggs', 'price': 1},\n",
       " {'name': 'coffee', 'price': 9.99},\n",
       " {'name': 'rice', 'price': 4.04}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def sort_by_price_ascending(json_string):\n",
    "    json_string = eval(json_string)\n",
    "    sorted_json = sorted(json_string, key=lambda x: (x[\"price\"], x[\"name\"]))\n",
    "    joined_json = \",\".join([json.dumps(i) for i in sorted_json])\n",
    "    print(str(joined_json))\n",
    "\n",
    "    final = \"[\" + joined_json + \"]\"\n",
    "\n",
    "    return final.replace(\" \", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\": \"eggs\", \"price\": 1},{\"name\": \"rice\", \"price\": 4.04},{\"name\": \"coffee\", \"price\": 9.99}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[{\"name\":\"eggs\",\"price\":1},{\"name\":\"rice\",\"price\":4.04},{\"name\":\"coffee\",\"price\":9.99}]'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp = sort_by_price_ascending(\n",
    "    '[{\"name\": \"eggs\",\"price\":1},{\"name\": \"coffee\",\"price\":9.99},{\"name\": \"rice\",\"price\":4.04}]'\n",
    ")\n",
    "pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010/02/20\n",
      "09/01/1994\n",
      "20100220\n",
      "19940109\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def transform_date_format(dates):\n",
    "    date = [d for d in dates if \"/\" in d or \"-\" in d]\n",
    "    date_list = []\n",
    "    for date_str in dates:\n",
    "        if re.match(r\"\\d{4}/(0[1-9]|1[0-2])/(0[1-9]|1[0-9]|2[0-9]|3[0-1])\", date_str):\n",
    "            print(date_str)\n",
    "            transformed_date = datetime.strptime(date_str, \"%Y/%m/%d\").strftime(\n",
    "                \"%Y%m%d\"\n",
    "            )\n",
    "            date_list.append(transformed_date)\n",
    "        elif re.match(r\"(0[1-9]|1[0-9]|2[0-9]|3[0-1])/(0[1-9]|1[0-2])/\\d{4}\", date_str):\n",
    "            print(date_str)\n",
    "            transformed_date = datetime.strptime(date_str, \"%d/%m/%Y\").strftime(\n",
    "                \"%Y%m%d\"\n",
    "            )\n",
    "            date_list.append(transformed_date)\n",
    "        elif re.match(r\"(0[1-9]|1[0-9]|2[0-9]|3[0-1])/(0[1-9]|1[0-2])/\\d{4}\", date_str):\n",
    "            print(date_str)\n",
    "            transformed_date = datetime.strptime(date_str, \"%m-%d-%Y\").strftime(\n",
    "                \"%Y%m%d\"\n",
    "            )\n",
    "            date_list.append(transformed_date)\n",
    "    return date_list\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dates = transform_date_format(\n",
    "        [\"2010/02/20\", \"09/01/1994\", \"10-09-1996\", \"20210221\"]\n",
    "    )\n",
    "    print(*dates, sep=\"\\n\")\n",
    "\n",
    "\n",
    "# dates = ['2010/02/20', '09/01/1994', '10-09-1996', '20210221']\n",
    "# sformed_dates = transform_date_format(dates)\n",
    "# print(sformed_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def transform_date_format(dates):\n",
    "    dates = [d for d in dates if \"/\" in d or \"-\" in d]\n",
    "    dates = [d for d in dates if len(d) == 10]\n",
    "    date_list = []\n",
    "\n",
    "    for date_str in dates:\n",
    "        if re.match(r\"\\d{4}/(0[1-9]|1[0-2])/(0[1-9]|1[0-9]|2[0-9]|3[0-1])\", date_str):\n",
    "            date_list.append(datetime.strptime(date_str, \"%d/%m/%Y\").strftime(\"%Y%m%d\"))\n",
    "        elif re.match(r\"(0[1-9]|1[0-9]|2[0-9]|3[0-1])/(0[1-9]|1[0-2])/\\d{4}\", date_str):\n",
    "            date_list.append(datetime.strptime(date_str, \"%d/%m/%Y\").strftime(\"%Y%m%d\"))\n",
    "        elif re.match(r\"(0[1-9]|1[0-9]|2[0-9]|3[0-1])/(0[1-9]|1[0-2])/\\d{4}\", date_str):\n",
    "            date_list.append(datetime.strptime(date_str, \"%d/%m/%Y\").strftime(\"%Y%m%d\"))\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return date_list\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dates = transform_date_format(\n",
    "        [\"2010/02/20\", \"09/01/1994\", \"10-09-1996\", \"20210221\"]\n",
    "    )\n",
    "    print(*dates, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def transform_date_format(dates):\n",
    "    dates = [d for d in dates if \"/\" in d or \"-\" in d]\n",
    "    dates = [d for d in dates if len(d) == 10]\n",
    "    date_list = []\n",
    "\n",
    "    for date_str in dates:\n",
    "        if re.match(r\"\\d{4}/(0[1-9]|1[0-2])/(0[1-9]|1[0-9]|2[0-9]|3[0-1])\", date_str):\n",
    "            transformed_date = datetime.strptime(date_str, \"%Y/%m/%d\").strftime(\n",
    "                \"%Y/%m/%d\"\n",
    "            )\n",
    "        elif re.match(r\"(0[1-9]|1[0-9]|2[0-9]|3[0-1])/(0[1-9]|1[0-2])/\\d{4}\", date_str):\n",
    "\n",
    "            transformed_date = datetime.strptime(date_str, \"%d/%m/%Y\").strftime(\n",
    "                \"%Y/%m/%d\"\n",
    "            )\n",
    "        elif re.match(r\"(0[1-9]|1[0-2])-(0[1-9]|1[0-9]|2[0-9]|3[0-1])-\\d{4}\", date_str):\n",
    "            transformed_date = datetime.strptime(date_str, \"%m/%d/%Y\").strftime(\n",
    "                \"%Y/%m/%d\"\n",
    "            )\n",
    "\n",
    "        date_list.append(transformed_date)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dates = transform_date_format(\n",
    "        [\"2010/02/20\", \"09/01/1994\", \"10-09-1996\", \"20210221\"]\n",
    "    )\n",
    "    print(*dates, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def transform_date_format(dates):\n",
    "    dates = [d for d in dates if \"/\" in d or \"-\" in d]\n",
    "    dates = [d for d in dates if len(d) == 10]\n",
    "    date_list = []\n",
    "    for date_str in dates:\n",
    "        if re.match(r\"\\d{4}/(0[1-9]|1[0-2])/(0[1-9]|1[0-9]|2[0-9]|3[0-1])\", date_str):\n",
    "            transformed_date = datetime.strptime(date_str, \"%Y/%m/%d\").strftime(\n",
    "                \"%Y%m%d\"\n",
    "            )\n",
    "            date_list.append(transformed_date)\n",
    "        elif re.match(r\"(0[1-9]|1[0-9]|2[0-9]|3[0-1])/(0[1-9]|1[0-2])/\\d{4}\", date_str):\n",
    "            transformed_date = datetime.strptime(date_str, \"%d/%m/%Y\").strftime(\n",
    "                \"%Y%m%d\"\n",
    "            )\n",
    "            date_list.append(transformed_date)\n",
    "        elif re.match(r\"(0[1-9]|1[0-2])-(0[1-9]|1[0-9]|2[0-9]|3[0-1])-\\d{4}\", date_str):\n",
    "            transformed_date = datetime.strptime(date_str, \"%d-%m-%Y\").strftime(\n",
    "                \"%Y%m%d\"\n",
    "            )\n",
    "            date_list.append(transformed_date)\n",
    "        else:\n",
    "            pass\n",
    "    return date_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
