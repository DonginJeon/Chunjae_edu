# 머신러닝

## 머신러닝 용어

- AI : 컴퓨터에 인간의 지능(직관)을 인공적으로 구현하는 방법 >>> 인간의 지능이 가지는 특성을 구현

  - 인간 지능의 특성
    1. 학습 : 직/간접적 경험(train set), 훈련(model fit)에 의해서 행동변화(가중치 변화)를 일으키는 현상
    2. 추리 : 어떤 판단의 전제 -> 새로운 판단
    3. 적응 : 환경에 따른 변화
    4. 논증 : 어떤 판단이 있을 때, 그 이유를 설명하는 것

- 머신러닝 : **학습 과정**을 거쳐서 어떤 데이터를 분류, 예측 등의 작업을 수행할 수 있다. >>> 컴퓨터가 f(x)를 구하는 과정

- 딥러닝 : 인간의 뉴런을 본따 인간의 신경망을 인공적으로 구현한 것(머신러닝 모델의 하위집합). 은닉층 >= 3

## 머신러닝의 분류

### 학습 과정에 따른 분류

1. 지도학습 : 훈련 데이터에 정답(레이블, y)이 포함된 경우
2. 비지도학습 : 훈련 데이터에 정답이 없는 경우
3. 강화학습 : 행동 - 보상 관계에 따른 학습
4. 준지도학습 : 일부 정답 있는 데이터를 학습

### 역할에 따른 분류

1. 분류 : 특정 클래스가 있을 때 주어진 데이터는 어떤 클래스에 속하는가를 예측

   > X : 범주형, 연속형 모두 가능
   > y : 범주형만 가능

   - 분류 모델 - 의사결정나무

     - 특성 : 전처리가 거의 필요없음. 복잡도가 데이터의 거의 정비례. 과적합이 쉽게 발생. NP-완전 문제
     - 예시코드

     ```python
     from sklearn.tree import DecisionTreeClassifier, plot_tree
     from sklearn.datasets import load_iris
     from sklearn.model_selection import train_test_split
     from sklearn import metrics

     iris_data = load_iris()
     X_train, X_test, y_train, y_test = train_test_split(
         iris_data.data, iris_data.target, test_size=0.2, random_state=12345
     )

     # DecisionTree Classifier
     dt_clf = DecisionTreeClassifier(random_state=12345)
     dt_clf.fit(X_train, y_train)
     y_pred = dt_clf.predict(X_test)
     accuracy = metrics.accuracy_score(y_test, y_pred)
     matrix = metrics.confusion_matrix(y_test, y_pred)

     ```

2. 회귀 : 주어진 데이터를 이용해서, 특정 시점(값)이 주어지는 경우, y는 어느정도의 값이 나오는지를 예측

   > y : 연속형

   - 회귀 모델 - 선형회귀

     - 특성 : 선형성, 등분산성을 띠는 데이터에만 이용가능. 오차의 독립성, 다중공선성이 없어야 한다.
     - 예시코드

     ```python
     import pandas as pd
     from sklearn.linear_model import LinearRegression
     import seaborn as sns
     reg = LinearRegression()

     crab = pd.read_csv('./data/CrabAgePrediction.csv')
     sns.regplot(data=crab, x='Length', y='Age', marker='+')
     reg.fit(crab['Length'].values.reshape(-1, 1), crab['Age'])
     reg.predict([[3]])
     ```


        ```

3. 군집화 : 어떤 데이터가 주어졌을 때 비슷한 특성의 데이터를 그룹화

   > y: 없음

   - 군집화 모델 - GMM

     - 특성 : 정규분포에 의존
     - 예시코드

     ```python
     from sklearn.mixture import GaussianMixture
     from sklearn.datasets import load_iris
     from sklearn.decomposition import PCA
     import seaborn as sns
     import pandas as pd

     pca = PCA(n_components=2) # 2차원으로 줄이는 코드

     iris_pca = pca.fit_transform(iris.data)
     gmm = GaussianMixture(n_components = 3, random_state = 12345)
     gmm_y = gmm.fit_predict(iris_pca)

     df = pd.DataFrame(iris_pca)
     sns.scatterplot(data = df, x=0, y=1, hue = gmm_y)
     ```

## 머신러닝의 장단점

### 장점

- 데이터의 패턴을 쉽게 찾아낼 수 있다.
- 자동화
- 성능 개선

### 단점

- 편향 : 데이터가 클래스가 나눠져 있을때 비율이 1:10이면 다 10인 클래스로 찍어버림 -> 리턴값도 편향된 값으로 나옴
- 데이터 수집
- 컴퓨팅 파워(자원문제 - 잠재적 손해)

## 머신러닝을 배울 때 필요한 지식

- 통계학(확률, 추론, 표집)
- 수학(선형대수학, 이산수학, 미적분), 컴퓨터과학(최적화, 알고리즘)

## 머신러닝 라이브러리

- sklearn, Tensorflow, PyTorch, Transformers, diffusers

## 데이터 전처리

- 머신러닝 모델은 y=f(x) 형태. >>> x,y 데이터의 정의가 필요
- y 데이터 : x 데이터와 어떤 관계(상관관계 등)를 가져야 함. 분류 모델의 경우, 각 클래스별로 개수가 균일해야 함.(균일하지 않은 경우 imbalanced data)
- x 데이터
  - 결측치가 없거나 적어야 함
  - 이상치가 없거나 적어야 함
  - 가능하면 정규분포를 띠는 것이 좋다
  - 머신러닝 모델은 0~1 사이의 값으로 x를 제한하는 것이 유리하다. (e.g. 기울기 소멸 문제)
  - x 데이터 사이의 공선성이 없어야 한다.
  - x 데이터의 차원이 지나치게 높거나 낮으면 안된다.

## 데이터편향

- (주로 분류 모델에서) 특정 클래스의 개수가 다른 클래스에 비해서 압도적으로 많은 경우, 머신러닝 모델이 절대다수의 클래스 정보를 지나치게 많이 학습할 수 있다.

  > 예시: A,B 클래스를 분류하려 할 때, A:B = 9990:10이라고 하자. 그러면 이 데이터를 학습한 기계학습 모델이 모든 정답을 A로 예측했다고 하면 정확도는 99.9%로 나타난다. 이 모델은 성능이 좋은가?
  >
  > > 이 문제를 해결하기 위해 정확도를 제외한 다른 평가 지표가 필요함

- 다른 평가 지표
  - 혼돈행렬 : 예측값과 실제값을 표로 표현한 것
    - TP : 예측(T) == 실제(T)
    - TN : 예측(N) == 실제(N)
    - FP : 예측(T) != 실제(N)
    - FN : 예측(N) != 실제(T)
  - 모델의 평가 지표
    - 정확도 : (TP+TN)/(TP+TN+FP+FN)
    - 민감도 : TP/(TP+FN) == 재현율
    - 특이도 : TN/(FP+TN)
    - 정밀도 : TP/(TP+FP)
    - F1 score : 2*(재현율*정밀도)/(재현율 + 정밀도)

## 교차검증(cross validation)

- 교차검증 : 데이터의 다양한 부분 집합을 사용해서 모델을 테스트하는 검증 기술
- 교차검증의 사용 목적: 과적합 방지, 분석 모델의 일반화

  ### K-fold CV

  - K-fold CV : 데이터를 k개로 나눈 뒤 k-1개 집합을 학습 데이터로 1개를 테스트 데이터로 활용하는 방법

    > 이때, k개로 나눈 데이터는 k가지 학습/테스트 데이터로 조합할 수 있다. >>> 검증 과정은 총 k번 진행된다.

  - 정점 : 같은 데이터 양으로 더욱 일반화된 결과를 확인할 수 있다. 데이터의 양을 증강시키는 것과 유사한 효과를 볼 수 있다.
  - 단점 : 학습량이 많아진다. 표집방법에 따라 편향이 생길 수 있다.
  - 예시코드

  ```python
  # 위에서 실행한 의사결정나무 코드를 활용
  # 5-fold CV
  import numpy as np
  from sklearn.model_selection import KFold
  kfold = KFold(n_splits = 5, shuffle = False)
  cv_ind = kfold.split(iris.data)

  cv_accu = []
  ind = 1

  for train_ind, test_ind, in cv_ind:
      X_train, X_test = iris.data[train_ind], iris.data[test_ind]
      y_train, y_test = iris.target[train_ind], iris.target[test_ind]

      dt =DecisionTreeClassifier(random_state=12345)
      df.fit(X_train,y_train)
      accu = dt.score(X_test,y_test)
      cv_accu.append(accu)
      ind += 1
  ```

  ### Stratified K-fold CV

  - Stratified K-fold CV: k-fold cv 과정에서 클래스의 비율을 모집합과 같이 조절
  - 정점 : k-fold cv에 비해서 데이터 쏘림 현상이 적어질 것이다
  - 단점 : 임의 표집이 아니기 때문에, 학습 과정에 이용자의 의도가 들어갈 수 있다
  - 예시코드

  ```python
  # 위에서 실행한 의사결정나무 코드를 활용

  import numpy as np
  from sklearn.model_selection import StratifiedKFold
  s_kfold = StratifiedKFold(n_splits = 5)
  cv_ind = s_kfold.split(iris.data, iris.target)

  cv_accu = []
  ind = 1

  for train_ind, test_ind, in cv_ind:
      X_train, X_test = iris.data[train_ind], iris.data[test_ind]
      y_train, y_test = iris.target[train_ind], iris.target[test_ind]

      dt =DecisionTreeClassifier(random_state=12345)
      df.fit(X_train,y_train)
      accu = dt.score(X_test,y_test)
      cv_accu.append(accu)
      ind += 1
  ```
